{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrnO8kzKrZU21WWuEQD6OD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranshikens/Epoch-Tasks/blob/main/Task_Session_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\Huge \\text{TASK SESSION-1}$**  \n",
        "<br>\n",
        "笳十tSupport Vector Machines (SVMs)  \n",
        "笳十tSingular Value Decomposition (SVD)  \n",
        "笳十tDimensionality reduction techniques (PCA, TSNE and more)  \n",
        "笳十tRandom Forests (including bagging and boosting)\n",
        "<br>  \n",
        "$\\large \\text{By - Saransh}$  \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ooCHn5jKJ_MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\huge \\text{STEP-1: DATA PROCESSING AND INITIAL INSIGHTS}$**"
      ],
      "metadata": {
        "id": "KoKGfPxjGTxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\LARGE \\text{Loading the Datasets}$**  \n",
        "I will be using Kaggle's API machinery to load the datasets."
      ],
      "metadata": {
        "id": "-BrR-lqxShll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up Kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hRRxg9JSPb3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jessicali9530/lfw-dataset\n",
        "!kaggle datasets download -d davilsena/ckdataset"
      ],
      "metadata": {
        "id": "JEM4-_VhPkVp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip lfw-dataset.zip\n",
        "!unzip ckdataset.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Vk1lVrA5PvCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwngJ9Wg_5Jq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uzf3mzxYSNmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\LARGE \\text{Reading the labelled data}$**"
      ],
      "metadata": {
        "id": "YzIF9XJsVDwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_data = pd.read_csv(\"/content/ckextended.csv\")\n",
        "small_data.head()"
      ],
      "metadata": {
        "id": "nI31OW-4DHAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way the pixels have been stored is problematic.  \n",
        "Each image has been reshaped to a vector, with all pixels stored in a single string.  \n",
        "For our purpose, each pixel has to be a separate feature, i.e. we need to store  \n",
        "each pixel as an individual feature and as an integer, not a string."
      ],
      "metadata": {
        "id": "U-X9ckzfNuGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\LARGE \\text{Processing the Labelled Data}$**"
      ],
      "metadata": {
        "id": "6Y84IWfGVKKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Usage' serves no \"use\" for the task at hand\n",
        "small_data = small_data.drop(columns=['Usage'])\n",
        "\n",
        "# splits each string entry in small_data['pixels'], and returns a list of individual pixel values for each entry\n",
        "small_data['pixels'] = small_data['pixels'].apply(lambda x: list(map(int, x.split())))\n",
        "\n",
        "# each \"pixel\" in small_data['pixels'] needs to be stored as a separate feature, since we want to describe an image as a vector\n",
        "pixels_df = pd.DataFrame(small_data['pixels'].tolist())\n",
        "\n",
        "# the original string of pixels is no longer needed\n",
        "small_data = small_data.drop(columns=['pixels'])\n",
        "small_data = pd.concat([small_data, pixels_df], axis=1)\n",
        "small_data.head()"
      ],
      "metadata": {
        "id": "0NJFZIqDDVmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0_joH2h_SRlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain more insights about the distribution of the emotions, we count the frequency of each emotion."
      ],
      "metadata": {
        "id": "rxGlZbA2PcmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_counts = small_data['emotion'].value_counts()\n",
        "emotion_counts"
      ],
      "metadata": {
        "id": "qZQ4XKTRf0HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above table reveals a problem with the dataset.  \n",
        "It is very imbalanced, with emotion '6' having more than 60% of the entries.  \n",
        "This will give an unfair weightage to emotion '6' when we try to fit a model to  \n",
        "this data, and try to access its accuracy and other metrics.  \n",
        "Basically, if our model works only for emotion '6', even then its accuracy will be higher than it realistically should be."
      ],
      "metadata": {
        "id": "Rf-RGKtQgL0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take care of this using two approaches:"
      ],
      "metadata": {
        "id": "uuVZgTHrgzww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\Large \\text{Approach-1: Dropping Emotion'6'}$**  \n",
        "We can ignore the entries that are causing trouble.  "
      ],
      "metadata": {
        "id": "3CB1SPHUg51X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\Large \\text{Approach-2: Randomly selecting entries for emotion '6'}$**  \n",
        "Ignoring emotion '6', each emotion has on an average 47 entries.  \n",
        "We may randomly select 47 entries for emotion '6', fit our model, repeat the process a certain number of times, and average out the results.  \n",
        "\n"
      ],
      "metadata": {
        "id": "KmvwZmeLiMCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RohYy24JSTtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\LARGE \\text{Reading and Processing the Unlabelled data}$**"
      ],
      "metadata": {
        "id": "rQoHPcbRR1Or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "Images have been stored in the \"lfw-deepfunneled\" folder as follows:  \n",
        "\"**lfw-deepfunneled**\" -> \"**lfw-deepfunneled**\" -> **Person's Name (Format: {FirstName}_{Surname} )** -> **image.jpg**  \n",
        "We loop through the folders/directories, searching for .jpg images.  \n",
        "When we encounter an image, we apply the these transformations:  \n",
        "\n",
        "\n",
        "1.   Convert the image to grayscale.\n",
        "2.   We want all our images (labelled and unlabelled both) to have the same size.  \n",
        " Since images in labelled data are 48 $\\times$ 48, we resize these encountered images to the same.\n",
        "3.   Currently, the image is a 48 $\\times$ 48 matrix. To store the images in a single 2-D dataframe, we reshape the 48 $\\times$ 48 matrix into a 1 $\\times$ 2304 vector.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sbuxWETVVQJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lfw_dir = \"/content/lfw-deepfunneled/lfw-deepfunneled\"\n",
        "\n",
        "image_vectors = [] # we will store each image here, and later use it to make a pandas dataframe for the unlabelled data\n",
        "\n",
        "for person_name in os.listdir(lfw_dir): # os.listdir(lfw_dir) returns a list of all files and folders in lfw_dir\n",
        "    person_dir = os.path.join(lfw_dir, person_name) # we obtain the path of each folder by joining its name with lfw_dir\n",
        "    if os.path.isdir(person_dir):\n",
        "        for image_filename in os.listdir(person_dir):\n",
        "            image_path = os.path.join(person_dir, image_filename)\n",
        "\n",
        "            # Open the image\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "            # Convert to grayscale\n",
        "            img_gray = img.convert('L')\n",
        "\n",
        "            # Resize to 48x48\n",
        "            img_resized = img_gray.resize((48, 48))\n",
        "\n",
        "            # Convert to numpy array and flatten into a 1x2304 vector\n",
        "            img_array = np.array(img_resized)\n",
        "            img_vector = img_array.flatten().tolist()\n",
        "\n",
        "            image_vectors.append(img_vector)\n",
        "\n",
        "\n",
        "large_data = pd.DataFrame(image_vectors)\n",
        "large_data.head()"
      ],
      "metadata": {
        "id": "ldwQ-shaRlP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FVTD9gb_Tn7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\LARGE \\text{Singular Value Decomposition on Large Data}$**"
      ],
      "metadata": {
        "id": "mTk7o5W4WaKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Both of our data has 2304 features. Using all of them will be computationally  \n",
        "expensive. Instead we can try to use the most significant features only."
      ],
      "metadata": {
        "id": "tNACbNAyumnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = large_data - large_data.mean()\n",
        "U, S, VT = np.linalg.svd(X, full_matrices=False)\n",
        "\n",
        "# S is a diagonal matrix. The diagonal elements in S give us the proportion in\n",
        "# which each component explains the variance of the original data. For convenience\n",
        "# and interpretability we transform the proportions into ratios.\n",
        "S = (S/np.sum(S))*100\n",
        "\n",
        "S_df = pd.DataFrame({\n",
        "    'Component': [f'S{i+1}' for i in range(len(S))],\n",
        "    'Percent Explained Variance': np.round(S, 2)\n",
        "})"
      ],
      "metadata": {
        "id": "Eujl9nvCuruV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\LARGE \\text{Plotting } S_{df}$**"
      ],
      "metadata": {
        "id": "z0Yy06UjVDyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will find out the distribution of the variance explained by each component in S_df.  \n",
        "This will help us in deducing which \"features\" are the most important in describing the images in large_data.  \n",
        "From here, we can reduce the dimensionality from 2304 to a significantly smaller number."
      ],
      "metadata": {
        "id": "9Ea-WMygdxyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(data=S_df, x='Component', y='Percent Explained Variance')\n",
        "plt.title(\"Percentage of Variance explained by each Component\")\n",
        "plt.xticks(ticks=range(0, 2304, 250))\n",
        "plt.ylabel(\"Explained Variance (in %)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n2TnEtK-dGMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow point of the distribution lies somewhere between 80 and 100.  \n",
        "Instead of using all of the 2304 features, we will use only 100, reducing our  \n",
        "load by more than 95%."
      ],
      "metadata": {
        "id": "niEFCR7leSmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eysPXpDkWCY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\huge \\text{STEP-2: FEARURE ENGINEERING WITH PCA}$**"
      ],
      "metadata": {
        "id": "KYSMkFYFe8Po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Principal Component Analysis on the Unlabelled Data, we will extract the  \n",
        "before mentioned 100 features."
      ],
      "metadata": {
        "id": "k4LN22ikfiMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_large_data = scaler.fit_transform(large_data)\n",
        "\n",
        "pca = PCA(n_components=100)\n",
        "large_pca_data = pca.fit_transform(scaled_large_data)\n",
        "\n",
        "large_pca_data_df = pd.DataFrame(large_pca_data, columns=[f'PC{i+1}' for i in range(large_pca_data.shape[1])])\n",
        "\n",
        "# comparing the importance of the extracted features\n",
        "explained_variance = pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "7n9z4mgAe7vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(x=range(1, len(explained_variance)+1), y=explained_variance*100)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Percent Explained Variance')\n",
        "plt.title('Percent Explained Variance for each Principal Component')\n",
        "plt.xticks(range(0, len(explained_variance)+1, 10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0M2WWaWjWNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fGyXAg26XcLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\huge \\text{STEP-3: SVM CLASSIFICATION AND EVALUATION}$**"
      ],
      "metadata": {
        "id": "tpHBBimnZi5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\LARGE \\text{Accounting for the Disbalance}$**"
      ],
      "metadata": {
        "id": "LDQBkS-Nrgi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\large \\text{APPROACH-1}$**"
      ],
      "metadata": {
        "id": "K30YMNsFrls0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the entries for which emotion=6\n",
        "small_data_1 = small_data[small_data['emotion']!=6]\n",
        "\n",
        "small_data_1 = small_data_1.drop(columns=['emotion'])\n",
        "\n",
        "# we use our pre-trained pca model to transform small_data_1 into a low dimensionality space\n",
        "small_data_1_pca = pca.transform(small_data_1)\n",
        "\n",
        "# we name our features PC1, PC2, and so on...\n",
        "small_data_1_pca_df = pd.DataFrame(small_data_1_pca, columns=[f'PC{i+1}' for i in range(small_data_1_pca.shape[1])])"
      ],
      "metadata": {
        "id": "TJoAwpEKrnrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "74T-qxfBYT5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca_1 = small_data_1_pca_df\n",
        "\n",
        "# in accordance with approach-1, we remove the entries where emotion=6\n",
        "y_pca_1 = small_data.loc[small_data['emotion'] != 6, 'emotion']\n",
        "\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_pca_1, y_pca_1, test_size=0.3, random_state=0, stratify=y_pca_1)\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "# we use different kernels for SVM, and use the best one out of them\n",
        "hyper_param_grid = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': [0.001, 0.1, 0.1],\n",
        "    'gamma': ['scale', 0.01, 0.001]\n",
        "}\n",
        "\n",
        "# cross-validation\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "\n",
        "# search through the grid of hyper parameters, and use cross validation to return the best set out of them\n",
        "grid_search = GridSearchCV(svm, hyper_param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train_1, y_train_1)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "FFABItqgalaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we choose the best choices for the hyperparameters, and use them in our SVM model\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "y_pred_1 = best_svm.predict(X_test_1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_1, y_pred_1))"
      ],
      "metadata": {
        "id": "GnzBVQakbeU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_1, y_pred_1)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_pred_1))\n",
        "display.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DMA6sO93b4xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vxXBeUYTa0Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\large \\text{APPROACH-2}$**"
      ],
      "metadata": {
        "id": "eBIMB7aPuwnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will consider a single iteration and afterwards compare the result with  \n",
        "the model using 100 iterations."
      ],
      "metadata": {
        "id": "H2e1_RTIKBzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we separate the entries with emotion 6\n",
        "emotion_6_data = small_data[small_data['emotion'] == 6]\n",
        "\n",
        "# selecting 47 random entries from emotion 6 data\n",
        "random_emotion_6 = emotion_6_data.sample(n=47, random_state=0)\n",
        "\n",
        "# getting the data where emotion is not 6\n",
        "rest_of_data = small_data[small_data['emotion'] != 6]\n",
        "\n",
        "# we combine the randomly selected emotion 6 data with the rest of the data\n",
        "small_data_2 = pd.concat([random_emotion_6, rest_of_data])"
      ],
      "metadata": {
        "id": "jz_Aln2ou0Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_data_2_dropped = small_data_2.drop(columns=['emotion'])\n",
        "\n",
        "# we use our pre-trained pca model to transform small_data_2 into a low dimensionality space\n",
        "small_data_2_pca = pca.transform(small_data_2_dropped)\n",
        "\n",
        "# we name our features PC1, PC2, and so on...\n",
        "small_data_2_pca_df = pd.DataFrame(small_data_2_pca, columns=[f'PC{i+1}' for i in range(small_data_2_pca.shape[1])])"
      ],
      "metadata": {
        "id": "jZnVAr8ywjYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca_2 = small_data_2_pca_df\n",
        "y_pca_2 = small_data_2['emotion']\n",
        "\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_pca_2, y_pca_2, test_size=0.3, random_state=0, stratify=y_pca_2)\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "# we use different kernels for SVM, and use the best one out of them\n",
        "hyper_param_grid = {\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'C': [0.001, 0.1, 0.1],\n",
        "    'gamma': ['scale', 0.01, 0.001]\n",
        "}\n",
        "\n",
        "# cross-validation\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "\n",
        "# search through the grid of hyper parameters, and use cross validation to return the best set out of them\n",
        "grid_search = GridSearchCV(svm, hyper_param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train_2, y_train_2)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "lIq7Bqb6w1eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we choose the best choices for the hyperparameters, and use them in our SVM model\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "y_pred_2 = best_svm.predict(X_test_2)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_2, y_pred_2))"
      ],
      "metadata": {
        "id": "iKPjJ0NKxXgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_2, y_pred_2)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_pred_2))\n",
        "display.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EcAjMT_Sxes5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qEJeq-F1KnsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\large \\text{INITIALIZING THE EVALUATION METRICS}$**"
      ],
      "metadata": {
        "id": "OevzEGCfN3cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will store results from each iteration in these lists and dictionaries\n",
        "accuracy_scores = []\n",
        "confusion_matrices = []\n",
        "\n",
        "# since we have to output the metrics for each emotion class, we initialize the keys\n",
        "# as the emotions themselves\n",
        "precision_scores = {str(i): [] for i in range(7)}\n",
        "recall_scores = {str(i): [] for i in range(7)}\n",
        "f1_scores = {str(i): [] for i in range(7)}\n",
        "\n",
        "n_iterations = 100"
      ],
      "metadata": {
        "id": "nnzahqy6N0wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WQtMhTSdOM96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$ \\large \\text{RUNNING THE ITERATIONS}$**"
      ],
      "metadata": {
        "id": "yL-lgjyvOOFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_iterations):\n",
        "    if (i+1)%10==0:\n",
        "        print(f\"Iteration {i+1}/{n_iterations}\")\n",
        "\n",
        "    # we separate the entries with emotion 6\n",
        "    emotion_6_data = small_data[small_data['emotion'] == 6]\n",
        "\n",
        "    # selecting 47 random entries from emotion 6 data\n",
        "    # we will use a different random state for each iteration\n",
        "    random_emotion_6 = emotion_6_data.sample(n=47, random_state=i)\n",
        "\n",
        "    # geting the data where emotion is not 6\n",
        "    rest_of_data = small_data[small_data['emotion'] != 6]\n",
        "\n",
        "    # we combine the randomly selected emotion 6 data with the rest of the data\n",
        "    small_data_2 = pd.concat([random_emotion_6, rest_of_data])\n",
        "\n",
        "    # Apply PCA transformation\n",
        "    small_data_2_dropped = small_data_2.drop(columns=['emotion'])\n",
        "\n",
        "    # we use our pre-trained pca model to transform small_data_2 into a low dimensionality space\n",
        "    small_data_2_pca = pca.transform(scaler.transform(small_data_2_dropped))\n",
        "\n",
        "    # we name our features PC1, PC2, and so on...\n",
        "    small_data_2_pca_df = pd.DataFrame(small_data_2_pca, columns=[f'PC{j+1}' for j in range(small_data_2_pca.shape[1])])\n",
        "\n",
        "    X_pca_2 = small_data_2_pca_df\n",
        "    y_pca_2 = small_data_2['emotion']\n",
        "\n",
        "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_pca_2, y_pca_2, test_size=0.3, random_state=0, stratify=y_pca_2)\n",
        "\n",
        "    svm = SVC()\n",
        "\n",
        "    # we use different kernels for SVM, and use the best one out of them\n",
        "    hyper_param_grid = {\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'C': [0.001, 0.1, 0.1],\n",
        "        'gamma': ['scale', 0.01, 0.001]\n",
        "    }\n",
        "\n",
        "    # cross-validation\n",
        "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "\n",
        "    # search through the grid of hyper parameters, and use cross validation to return the best set out of them\n",
        "    grid_search = GridSearchCV(svm, hyper_param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    grid_search.fit(X_train_2, y_train_2)\n",
        "\n",
        "    best_svm = grid_search.best_estimator_\n",
        "\n",
        "    y_pred_2 = best_svm.predict(X_test_2)\n",
        "\n",
        "    # we will calculate all the evaluation metrics and store them in the lists\n",
        "    report = classification_report(y_test_2, y_pred_2, output_dict=True)\n",
        "    accuracy_scores.append(report['accuracy'])\n",
        "    for label in precision_scores.keys():\n",
        "        precision_scores[label].append(report[label]['precision'])\n",
        "\n",
        "\n",
        "    for label in recall_scores.keys():\n",
        "         recall_scores[label].append(report[label]['recall'])\n",
        "\n",
        "\n",
        "    for label in f1_scores.keys():\n",
        "        f1_scores[label].append(report[label]['f1-score'])\n",
        "\n",
        "    # to determining the overall confusion matrix, we sum up all the confusion matrices\n",
        "    if i == 0:\n",
        "        overall_confusion_matrix = confusion_matrix(y_test_2, y_pred_2, labels=np.unique(y_pca_2))\n",
        "    else:\n",
        "        overall_confusion_matrix += confusion_matrix(y_test_2, y_pred_2, labels=np.unique(y_pca_2))"
      ],
      "metadata": {
        "id": "ZRwwxYIdy-WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$\\large \\text{AVERAGING OUT THE METRICS}$**"
      ],
      "metadata": {
        "id": "7aG2n1AYOjrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_confusion_matrix = overall_confusion_matrix / n_iterations\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "average_precision = {label: np.mean(precision_scores[label]) for label in precision_scores.keys()}\n",
        "average_recall = {label: np.mean(recall_scores[label]) for label in recall_scores.keys()}\n",
        "average_f1 = {label: np.mean(f1_scores[label]) for label in f1_scores.keys()}"
      ],
      "metadata": {
        "id": "N2oN3eAKOYga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gpp84J1FOe9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$ \\large \\text{THE AVERAGE CLASSIFICATION REPORT}$**"
      ],
      "metadata": {
        "id": "3IHZ5OEYOr6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nAverage Classification Report over {n_iterations} iterations:\")\n",
        "print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
        "print(\"Average Precision:\")\n",
        "for label, avg_prec in average_precision.items():\n",
        "    print(f\"  {label}: {avg_prec:.4f}\")\n",
        "print(\"Average Recall:\")\n",
        "for label, avg_rec in average_recall.items():\n",
        "    print(f\"  {label}: {avg_rec:.4f}\")\n",
        "print(\"Average F1-score:\")\n",
        "for label, avg_f1_score in average_f1.items():\n",
        "    print(f\"  {label}: {avg_f1_score:.4f}\")"
      ],
      "metadata": {
        "id": "W6AF_hnVOZiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "efLklrx9Nt1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$ \\large \\text{THE AVERAGE CONFUSION MATRIX}$**"
      ],
      "metadata": {
        "id": "yw8XNZqUO4_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display = ConfusionMatrixDisplay(confusion_matrix=average_confusion_matrix, display_labels=np.unique(y_pca_2))\n",
        "display.plot(cmap='Blues')\n",
        "plt.title(f\"Average Confusion Matrix over {n_iterations} Iterations\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ddUk0axOOb4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MbexMrqIPFmv"
      }
    }
  ]
}